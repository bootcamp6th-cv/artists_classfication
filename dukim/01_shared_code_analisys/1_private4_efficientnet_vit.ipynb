{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-12-11T07:45:14.524349Z",
     "start_time": "2023-12-11T07:45:14.381659100Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import math\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from scipy.stats import beta\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, sampler\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms.functional\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-11T07:45:14.647366800Z",
     "start_time": "2023-12-11T07:45:14.396292700Z"
    }
   },
   "id": "52bde1346932dd18"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Hyperparameter Setting"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9200015b3b978ea9"
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "CFG = {\n",
    "    'IMG_SIZE_B':384,\n",
    "    'IMG_SIZE_L':224,\n",
    "    'EPOCHS':400,\n",
    "    'LEARNING_RATE':0.00002,\n",
    "    'BATCH_SIZE':4,\n",
    "    'SEED':41\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-11T07:45:14.708020500Z",
     "start_time": "2023-12-11T07:45:14.415878800Z"
    }
   },
   "id": "de14e1fb25b2e50d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Fixed Random Seed"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5dc70322287fc136"
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed_everything(CFG['SEED']) # Seed 고정"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-11T07:45:14.765950100Z",
     "start_time": "2023-12-11T07:45:14.422877500Z"
    }
   },
   "id": "b958b1ab1e33a4a6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data Pre-processing"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "450d5df31bddc3f5"
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "data_path = '../../data/'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-11T07:45:14.817276300Z",
     "start_time": "2023-12-11T07:45:14.439898500Z"
    }
   },
   "id": "2c15d6a712007b57"
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(os.path.join(data_path, 'train.csv'))\n",
    "artists_df = pd.read_csv(os.path.join(data_path, 'artists_info.csv'))\n",
    "artists_df.loc[artists_df['name'] == 'Albrecht Dürer', 'name'] = 'Albrecht Du rer'\n",
    "train_df.loc[3896, 'artist'] = 'Titian'\n",
    "train_df.loc[3986, 'artist'] = 'Alfred Sisley'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-11T08:33:14.092746800Z",
     "start_time": "2023-12-11T08:33:14.046211700Z"
    }
   },
   "id": "1a7ca255149285a5"
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "train_df = pd.merge(train_df, artists_df.loc[:, ['name', 'genre']], left_on='artist', right_on='name', how='left').drop(columns='name')\n",
    "train_df.loc[train_df['artist']=='Diego Rivera', 'genre'] = 'Social Realism'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-11T07:45:14.858405100Z",
     "start_time": "2023-12-11T07:45:14.484758400Z"
    }
   },
   "id": "9435bc7892b622de"
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "train_df.to_csv(os.path.join(data_path, 'train2.csv'), index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-11T07:45:14.885077300Z",
     "start_time": "2023-12-11T07:45:14.517336600Z"
    }
   },
   "id": "9db38ee4ff3543af"
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "data": {
      "text/plain": "   id          img_path            artist                 genre\n0   0  ./train/0000.jpg   Diego Velazquez               Baroque\n1   1  ./train/0001.jpg  Vincent van Gogh    Post-Impressionism\n2   2  ./train/0002.jpg      Claude Monet         Impressionism\n3   3  ./train/0003.jpg       Edgar Degas         Impressionism\n4   4  ./train/0004.jpg  Hieronymus Bosch  Northern Renaissance",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>img_path</th>\n      <th>artist</th>\n      <th>genre</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>./train/0000.jpg</td>\n      <td>Diego Velazquez</td>\n      <td>Baroque</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>./train/0001.jpg</td>\n      <td>Vincent van Gogh</td>\n      <td>Post-Impressionism</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>./train/0002.jpg</td>\n      <td>Claude Monet</td>\n      <td>Impressionism</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>./train/0003.jpg</td>\n      <td>Edgar Degas</td>\n      <td>Impressionism</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>./train/0004.jpg</td>\n      <td>Hieronymus Bosch</td>\n      <td>Northern Renaissance</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(os.path.join(data_path, 'train2.csv'))\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-11T07:45:14.885937500Z",
     "start_time": "2023-12-11T07:45:14.548801200Z"
    }
   },
   "id": "7478e3052910f2c1"
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "def onehot_encoding_smoothing(x):\n",
    "    if x['High Renaissance,Mannerism'] == 1:\n",
    "        x['High Renaissance'] = 0.5\n",
    "        x['Mannerism'] = 0.5\n",
    "    elif x['Impressionism,Post-Impressionism'] == 1:\n",
    "        x['Impressionism'] = 0.5\n",
    "        x['Post-Impressionism'] = 0.5\n",
    "\n",
    "    return x\n",
    "\n",
    "def get_data(df, infer=False):\n",
    "    if infer:\n",
    "        return df['img_path'].apply(lambda p: os.path.join(data_path, p)).values\n",
    "\n",
    "    onehot_encoding = pd.get_dummies(df['genre'])\n",
    "    onehot_encoding = onehot_encoding.apply(lambda x: onehot_encoding_smoothing(x), axis=1)\n",
    "    onehot_encoding.drop(['High Renaissance,Mannerism', 'Impressionism,Post-Impressionism'], axis=1, inplace=True)\n",
    "    onehot_encoding = onehot_encoding.values\n",
    "\n",
    "    return df['img_path'].apply(lambda p: os.path.join(data_path, p)).values, df['artist'].values, onehot_encoding.astype(float)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-11T07:45:14.885937500Z",
     "start_time": "2023-12-11T07:45:14.581127900Z"
    }
   },
   "id": "ec188bb8b3c53bf8"
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "# Label Encoding\n",
    "le = preprocessing.LabelEncoder()\n",
    "df['artist'] = le.fit_transform(df['artist'].values)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-11T07:45:14.885937500Z",
     "start_time": "2023-12-11T07:45:14.592630900Z"
    }
   },
   "id": "94bada7449fad69d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train/Validation Split"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "be09bce99c10cc58"
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "train_df, val_df = train_test_split(df, test_size=0.2, stratify=df['artist'].values, random_state=CFG['SEED'])\n",
    "\n",
    "train_df = train_df.sort_values(by=['id'])\n",
    "val_df = val_df.sort_values(by=['id'])\n",
    "\n",
    "train_img_paths, train_labels, train_genre_labels = get_data(train_df)\n",
    "val_img_paths, val_labels, val_genre_labels = get_data(val_df)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-11T07:45:15.114621100Z",
     "start_time": "2023-12-11T07:45:14.608519800Z"
    }
   },
   "id": "442dfe7afecbb466"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# CustomDataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "870419344d59c5be"
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, img_paths, labels, genre_labels, transforms=None):\n",
    "        self.img_paths = img_paths\n",
    "        self.labels = labels\n",
    "        self.genre_labels = genre_labels\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_path = self.img_paths[index]\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            image = self.transforms(image=image)['image']\n",
    "\n",
    "        if self.labels is not None:\n",
    "            label = self.labels[index]\n",
    "            genre_label = self.genre_labels[index]\n",
    "            return (image, label, genre_label)\n",
    "        else:\n",
    "            return image\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-11T07:45:15.114621100Z",
     "start_time": "2023-12-11T07:45:15.016941Z"
    }
   },
   "id": "6815b5125057de50"
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "# EfficientNet_V2_M, ViT_B_16을 위한 변환기\n",
    "train_transform_b = A.Compose([\n",
    "    A.RandomResizedCrop(CFG['IMG_SIZE_B'], CFG['IMG_SIZE_B'], scale=(0.2, 0.8)),\n",
    "    A.Transpose(p=0.5),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.5),\n",
    "    A.ShiftScaleRotate(p=0.5),\n",
    "    A.HueSaturationValue(hue_shift_limit=20, sat_shift_limit=20, val_shift_limit=20, p=0.5),\n",
    "    A.RandomBrightnessContrast(brightness_limit=(-0.1,0.1), contrast_limit=(-0.1, 0.1), p=0.5),\n",
    "    A.ChannelShuffle(),\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), max_pixel_value=255.0, always_apply=False, p=1.0),\n",
    "    A.CoarseDropout(p=0.5),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "test_transform_b = A.Compose([\n",
    "    A.Resize(CFG['IMG_SIZE_B'],CFG['IMG_SIZE_B']),\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), max_pixel_value=255.0, always_apply=False, p=1.0),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "# ViT_L_16을 위한 변환기\n",
    "train_transform_l = A.Compose([\n",
    "    A.RandomResizedCrop(CFG['IMG_SIZE_L'], CFG['IMG_SIZE_L'], scale=(0.2, 0.8)),\n",
    "    A.Transpose(p=0.5),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.5),\n",
    "    A.ShiftScaleRotate(p=0.5),\n",
    "    A.HueSaturationValue(hue_shift_limit=20, sat_shift_limit=20, val_shift_limit=20, p=0.5),\n",
    "    A.RandomBrightnessContrast(brightness_limit=(-0.1,0.1), contrast_limit=(-0.1, 0.1), p=0.5),\n",
    "    A.ChannelShuffle(),\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), max_pixel_value=255.0, always_apply=False, p=1.0),\n",
    "    A.CoarseDropout(p=0.5),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "test_transform_l = A.Compose([\n",
    "    A.Resize(CFG['IMG_SIZE_L'],CFG['IMG_SIZE_L']),\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), max_pixel_value=255.0, always_apply=False, p=1.0),\n",
    "    ToTensorV2()\n",
    "])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-11T07:45:15.145585900Z",
     "start_time": "2023-12-11T07:45:15.036959200Z"
    }
   },
   "id": "f8ed677204e653af"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Weighted Random Sampling"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b82485484416851a"
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "def make_weights(labels, nclasses):\n",
    "    labels = np.array(labels)\n",
    "    weight_arr = np.zeros_like(labels)\n",
    "\n",
    "    _, counts = np.unique(labels, return_counts=True)\n",
    "    for cls in range(nclasses):\n",
    "        weight_arr = np.where(labels == cls, 1/counts[cls], weight_arr)\n",
    "        # 각 클래스의의 인덱스를 산출하여 해당 클래스 개수의 역수를 확률로 할당한다.\n",
    "        # 이를 통해 각 클래스의 전체 가중치를 동일하게 한다.\n",
    "\n",
    "    return weight_arr\n",
    "\n",
    "weights = make_weights(train_labels, len(np.unique(train_labels)))\n",
    "weights = torch.DoubleTensor(weights)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-11T07:45:15.146586100Z",
     "start_time": "2023-12-11T07:45:15.048474200Z"
    }
   },
   "id": "b73368abc97d8fc3"
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [],
   "source": [
    "# EfficientNet_V2_M, ViT_B_16을 위한 data loader\n",
    "train_dataset_b = CustomDataset(train_img_paths, train_labels, train_genre_labels, train_transform_b)\n",
    "train_loader_b = DataLoader(train_dataset_b, batch_size = CFG['BATCH_SIZE'], num_workers=0,\n",
    "                          sampler=sampler.WeightedRandomSampler(weights, len(weights)))\n",
    "\n",
    "val_dataset_b = CustomDataset(val_img_paths, val_labels, val_genre_labels, test_transform_b)\n",
    "val_loader_b = DataLoader(val_dataset_b, batch_size=CFG['BATCH_SIZE'], shuffle=False, num_workers=0)\n",
    "\n",
    "# ViT_L_16을 위한 data loader\n",
    "train_dataset_l = CustomDataset(train_img_paths, train_labels, train_genre_labels, train_transform_l)\n",
    "train_loader_l = DataLoader(train_dataset_l, batch_size = CFG['BATCH_SIZE'], num_workers=0,\n",
    "                          sampler=sampler.WeightedRandomSampler(weights, len(weights)))\n",
    "\n",
    "val_dataset_l = CustomDataset(val_img_paths, val_labels, val_genre_labels, test_transform_l)\n",
    "val_loader_l = DataLoader(val_dataset_l, batch_size=CFG['BATCH_SIZE'], shuffle=False, num_workers=0)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-11T07:45:15.146586100Z",
     "start_time": "2023-12-11T07:45:15.065492300Z"
    }
   },
   "id": "a1d41b33a9bfc9c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model Define\n",
    "* 장르 정보를 활용하지 않은 EfficientNet_V2_M\n",
    "* 장르 정보를 활용하지 않은 ViT_B_16\n",
    "* 장르 정보를 활용한 ViT_B_16\n",
    "* 장르 정보를 활용하지 않은 ViT_L_16\n",
    "* 장르 정보를 활용한 ViT_L_16"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1b157f68646457ac"
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [],
   "source": [
    "# EfficientNet_V2_M\n",
    "class BaseModel(nn.Module):\n",
    "    def __init__(self, num_classes=len(le.classes_)):\n",
    "        super(BaseModel, self).__init__()\n",
    "        self.backbone = models.efficientnet_v2_m(weights=models.EfficientNet_V2_M_Weights)\n",
    "        self.backbone.classifier = nn.Sequential(\n",
    "            nn.Dropout(p=0.3, inplace=True),\n",
    "            nn.Linear(in_features=1280, out_features=num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-11T07:45:15.147589900Z",
     "start_time": "2023-12-11T07:45:15.079529700Z"
    }
   },
   "id": "8a4d6c075c9b2046"
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([1, 577, 768])"
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ViT_B_16 & ViT_L_16\n",
    "# Attention 기법이 사용된 ViT 모델\n",
    "# torchvision.models.ViT_B_16_Weights.IMAGENET1K_SWAG_E2E_V1 기본 이미지 크기 = 384 \n",
    "# torchvision.models.ViT_L_16_Weights.IMAGENET1K_SWAG_LINEAR_V1 기본 이미지 크기 = 224\n",
    "models.vit_b_16(weights=models.ViT_B_16_Weights.IMAGENET1K_SWAG_E2E_V1).encoder.pos_embedding.size()\n",
    "# pos_embedding = (1, seq_length, hidden_dim) = (1, 577, 768)\n",
    "# 이미지 크기 384, 패치 크기 16\n",
    "# ... 이미지 패치 개수 = (384//16)**2 = 576\n",
    "# + 클래스 토큰 = 577"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-11T07:45:16.434156Z",
     "start_time": "2023-12-11T07:45:15.096553600Z"
    }
   },
   "id": "2e705971d30cdd8a"
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([1, 626, 768])"
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 이미지 크기 400, 패치크기 16으로 변경시\n",
    "# 이미지 패치 개수 = (400//16)**2 = 625\n",
    "# + 클래스 토큰1 = 626\n",
    "image_resize_test_model = models.vision_transformer.VisionTransformer(\n",
    "    image_size=400,\n",
    "    patch_size=16,\n",
    "    num_layers=12,\n",
    "    num_heads=12,\n",
    "    hidden_dim=768,\n",
    "    mlp_dim=3072,\n",
    ")\n",
    "image_resize_test_model.encoder.pos_embedding.size()\n",
    "# (1, 626, 768)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-11T07:45:17.105539100Z",
     "start_time": "2023-12-11T07:45:16.439168400Z"
    }
   },
   "id": "612f3b2c50a6d42e"
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "data": {
      "text/plain": "<All keys matched successfully>"
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "# ViT 논문 - 3.2 FINE-TUNING AND HIGHER RESOLUTION\n",
    "image_resize_test_model.load_state_dict(\n",
    "    torchvision.models.vision_transformer.interpolate_embeddings(\n",
    "        image_size=400,\n",
    "        patch_size=16,\n",
    "        model_state=OrderedDict(models.ViT_B_16_Weights.IMAGENET1K_SWAG_E2E_V1.get_state_dict(progress=True))\n",
    "    )\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-11T07:45:18.072526400Z",
     "start_time": "2023-12-11T07:45:17.108539900Z"
    }
   },
   "id": "45699f944796f293"
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [],
   "source": [
    "class ViTModelB16(nn.Module):\n",
    "    def __init__(self, num_classes=len(le.classes_)):\n",
    "        super(ViTModelB16, self).__init__()\n",
    "        self.backbone = models.vit_b_16(weights=models.ViT_B_16_Weights.IMAGENET1K_SWAG_E2E_V1)\n",
    "        self.backbone.heads = nn.Sequential(\n",
    "            nn.Linear(in_features=768, out_features=num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ViTModelL16(nn.Module):\n",
    "    def __init__(self, num_classes=len(le.classes_)):\n",
    "        super(ViTModelL16, self).__init__()\n",
    "        self.backbone = models.vit_l_16(weights=models.ViT_L_16_Weights.IMAGENET1K_SWAG_LINEAR_V1)\n",
    "        self.backbone.heads = nn.Sequential(\n",
    "            nn.Linear(in_features=1024, out_features=num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-11T07:45:18.090001500Z",
     "start_time": "2023-12-11T07:45:18.072526400Z"
    }
   },
   "id": "f281f38f0d933d0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# ViT_B_16_Genre & ViT_L_16_Genre"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "937ad111562b3741"
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [],
   "source": [
    "class ViTModelB16Genre(nn.Module):\n",
    "    def __init__(self, num_classes=len(le.classes_)):\n",
    "        super(ViTModelB16Genre, self).__init__()\n",
    "        self.backbone = models.vit_b_16(weights=models.ViT_B_16_Weights.IMAGENET1K_SWAG_E2E_V1)\n",
    "        self.backbone.heads = nn.Identity() # 기존 Classifer를 사용하지 않기 위해 입력값을 그대로 출력해주는 역할\n",
    "\n",
    "        self.genre_classifier = nn.Linear(in_features=768, out_features=23)\n",
    "\n",
    "        self.layer_norm = nn.LayerNorm(768 + 23, eps=1e-6)\n",
    "        self.artist_classifier = nn.Linear(in_features=768 + 23, out_features=num_classes)\n",
    "\n",
    "    def forward(self, x, genre_label=None):\n",
    "        x = self.backbone(x)\n",
    "        genre_pred = self.genre_classifier(x)\n",
    "\n",
    "        if genre_label is not None:\n",
    "            x = torch.cat([x, genre_label], dim=1)   # Teacher Forcing\n",
    "        else:\n",
    "            x = torch.cat([x, genre_pred], dim=1)\n",
    "\n",
    "        x = self.layer_norm(x)\n",
    "        x = self.artist_classifier(x)\n",
    "\n",
    "        return x, genre_pred\n",
    "\n",
    "\n",
    "class ViTModelL16Genre(nn.Module):\n",
    "    def __init__(self, num_classes=len(le.classes_)):\n",
    "        super(ViTModelL16Genre, self).__init__()\n",
    "        self.backbone = models.vit_l_16(weights=models.ViT_L_16_Weights.IMAGENET1K_SWAG_LINEAR_V1)\n",
    "        self.backbone.heads = nn.Identity()\n",
    "\n",
    "        self.genre_classifier = nn.Linear(in_features=1024, out_features=23)\n",
    "\n",
    "        self.layer_norm = nn.LayerNorm(1024 + 23, eps=1e-6)\n",
    "        self.artist_classifier = nn.Linear(in_features=1024 + 23, out_features=num_classes)\n",
    "\n",
    "    def forward(self, x, genre_label=None):\n",
    "        x = self.backbone(x)\n",
    "        genre_pred = self.genre_classifier(x)\n",
    "\n",
    "        if genre_label is not None:\n",
    "            x = torch.cat([x, genre_label], dim=1)   # Teacher Forcing\n",
    "        else:\n",
    "            x = torch.cat([x, genre_pred], dim=1)\n",
    "\n",
    "        x = self.layer_norm(x)\n",
    "        x = self.artist_classifier(x)\n",
    "\n",
    "        return x, genre_pred"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-11T07:45:18.135166900Z",
     "start_time": "2023-12-11T07:45:18.097497300Z"
    }
   },
   "id": "24722835eb953be"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Cutmix, FMix, MixUp\n",
    "* 데이터 증강 기법\n",
    "* Cutmix, FMix 참고링크 : https://www.kaggle.com/code/ar2017/pytorch-efficientnet-train-aug-cutmix-fmix\n",
    "* MixUp 참고링크 : https://dacon.io/competitions/official/235842/codeshare/3665"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cf86fa031b4379d9"
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [],
   "source": [
    "def rand_bbox(size, lam):\n",
    "    W = size[2]\n",
    "    H = size[3]\n",
    "    cut_rat = np.sqrt(1. - lam)\n",
    "    cut_w = int(W * cut_rat)\n",
    "    cut_h = int(H * cut_rat)\n",
    "\n",
    "    # uniform\n",
    "    cx = np.random.randint(W)\n",
    "    cy = np.random.randint(H)\n",
    "\n",
    "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
    "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
    "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
    "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
    "    return bbx1, bby1, bbx2, bby2\n",
    "\n",
    "\n",
    "def fftfreqnd(h, w=None, z=None):\n",
    "    \"\"\" Get bin values for discrete fourier transform of size (h, w, z)\n",
    "    :param h: Required, first dimension size\n",
    "    :param w: Optional, second dimension size\n",
    "    :param z: Optional, third dimension size\n",
    "    \"\"\"\n",
    "    fz = fx = 0\n",
    "    fy = np.fft.fftfreq(h)\n",
    "\n",
    "    if w is not None:\n",
    "        fy = np.expand_dims(fy, -1)\n",
    "\n",
    "        if w % 2 == 1:\n",
    "            fx = np.fft.fftfreq(w)[: w // 2 + 2]\n",
    "        else:\n",
    "            fx = np.fft.fftfreq(w)[: w // 2 + 1]\n",
    "\n",
    "    if z is not None:\n",
    "        fy = np.expand_dims(fy, -1)\n",
    "        if z % 2 == 1:\n",
    "            fz = np.fft.fftfreq(z)[:, None]\n",
    "        else:\n",
    "            fz = np.fft.fftfreq(z)[:, None]\n",
    "\n",
    "    return np.sqrt(fx * fx + fy * fy + fz * fz)\n",
    "\n",
    "\n",
    "def get_spectrum(freqs, decay_power, ch, h, w=0, z=0):\n",
    "    \"\"\" Samples a fourier image with given size and frequencies decayed by decay power\n",
    "    :param freqs: Bin values for the discrete fourier transform\n",
    "    :param decay_power: Decay power for frequency decay prop 1/f**d\n",
    "    :param ch: Number of channels for the resulting mask\n",
    "    :param h: Required, first dimension size\n",
    "    :param w: Optional, second dimension size\n",
    "    :param z: Optional, third dimension size\n",
    "    \"\"\"\n",
    "    scale = np.ones(1) / (np.maximum(freqs, np.array([1. / max(w, h, z)])) ** decay_power)\n",
    "\n",
    "    param_size = [ch] + list(freqs.shape) + [2]\n",
    "    param = np.random.randn(*param_size)\n",
    "\n",
    "    scale = np.expand_dims(scale, -1)[None, :]\n",
    "\n",
    "    return scale * param\n",
    "\n",
    "\n",
    "def make_low_freq_image(decay, shape, ch=1):\n",
    "    \"\"\" Sample a low frequency image from fourier space\n",
    "    :param decay_power: Decay power for frequency decay prop 1/f**d\n",
    "    :param shape: Shape of desired mask, list up to 3 dims\n",
    "    :param ch: Number of channels for desired mask\n",
    "    \"\"\"\n",
    "    freqs = fftfreqnd(*shape)\n",
    "    spectrum = get_spectrum(freqs, decay, ch, *shape)#.reshape((1, *shape[:-1], -1))\n",
    "    spectrum = spectrum[:, 0] + 1j * spectrum[:, 1]\n",
    "    mask = np.real(np.fft.irfftn(spectrum, shape))\n",
    "\n",
    "    if len(shape) == 1:\n",
    "        mask = mask[:1, :shape[0]]\n",
    "    if len(shape) == 2:\n",
    "        mask = mask[:1, :shape[0], :shape[1]]\n",
    "    if len(shape) == 3:\n",
    "        mask = mask[:1, :shape[0], :shape[1], :shape[2]]\n",
    "\n",
    "    mask = mask\n",
    "    mask = (mask - mask.min())\n",
    "    mask = mask / mask.max()\n",
    "    return mask\n",
    "\n",
    "\n",
    "def sample_lam(alpha, reformulate=False):\n",
    "    \"\"\" Sample a lambda from symmetric beta distribution with given alpha\n",
    "    :param alpha: Alpha value for beta distribution\n",
    "    :param reformulate: If True, uses the reformulation of [1].\n",
    "    \"\"\"\n",
    "    if reformulate:\n",
    "        lam = beta.rvs(alpha+1, alpha)\n",
    "    else:\n",
    "        lam = beta.rvs(alpha, alpha)\n",
    "\n",
    "    return lam\n",
    "\n",
    "\n",
    "def binarise_mask(mask, lam, in_shape, max_soft=0.0):\n",
    "    \"\"\" Binarises a given low frequency image such that it has mean lambda.\n",
    "    :param mask: Low frequency image, usually the result of `make_low_freq_image`\n",
    "    :param lam: Mean value of final mask\n",
    "    :param in_shape: Shape of inputs\n",
    "    :param max_soft: Softening value between 0 and 0.5 which smooths hard edges in the mask.\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    idx = mask.reshape(-1).argsort()[::-1]\n",
    "    mask = mask.reshape(-1)\n",
    "    num = math.ceil(lam * mask.size) if random.random() > 0.5 else math.floor(lam * mask.size)\n",
    "\n",
    "    eff_soft = max_soft\n",
    "    if max_soft > lam or max_soft > (1-lam):\n",
    "        eff_soft = min(lam, 1-lam)\n",
    "\n",
    "    soft = int(mask.size * eff_soft)\n",
    "    num_low = num - soft\n",
    "    num_high = num + soft\n",
    "\n",
    "    mask[idx[:num_high]] = 1\n",
    "    mask[idx[num_low:]] = 0\n",
    "    mask[idx[num_low:num_high]] = np.linspace(1, 0, (num_high - num_low))\n",
    "\n",
    "    mask = mask.reshape((1, *in_shape))\n",
    "    return mask\n",
    "\n",
    "\n",
    "def sample_mask(alpha, decay_power, shape, max_soft=0.0, reformulate=False):\n",
    "    \"\"\" Samples a mean lambda from beta distribution parametrised by alpha, creates a low frequency image and binarises\n",
    "    it based on this lambda\n",
    "    :param alpha: Alpha value for beta distribution from which to sample mean of mask\n",
    "    :param decay_power: Decay power for frequency decay prop 1/f**d\n",
    "    :param shape: Shape of desired mask, list up to 3 dims\n",
    "    :param max_soft: Softening value between 0 and 0.5 which smooths hard edges in the mask.\n",
    "    :param reformulate: If True, uses the reformulation of [1].\n",
    "    \"\"\"\n",
    "    if isinstance(shape, int):\n",
    "        shape = (shape,)\n",
    "\n",
    "    # Choose lambda\n",
    "    lam = sample_lam(alpha, reformulate)\n",
    "\n",
    "    # Make mask, get mean / std\n",
    "    mask = make_low_freq_image(decay_power, shape)\n",
    "    mask = binarise_mask(mask, lam, shape, max_soft)\n",
    "\n",
    "    return lam, mask"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-11T09:29:32.392216400Z",
     "start_time": "2023-12-11T09:29:32.341963700Z"
    }
   },
   "id": "4cb8403578af23d"
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [],
   "source": [
    "def cutmix(data, image_target, genre_target, alpha):\n",
    "    indices = torch.randperm(data.size(0))\n",
    "    shuffled_data = data[indices]\n",
    "    shuffled_image_target = image_target[indices]\n",
    "    shuffled_genre_target = genre_target[indices]\n",
    "\n",
    "    lam = np.clip(np.random.beta(alpha, alpha),0.3,0.4)\n",
    "    bbx1, bby1, bbx2, bby2 = rand_bbox(data.size(), lam)\n",
    "    new_data = data.clone()\n",
    "    new_data[:, :, bby1:bby2, bbx1:bbx2] = data[indices, :, bby1:bby2, bbx1:bbx2]\n",
    "    # adjust lambda to exactly match pixel ratio\n",
    "    lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (data.size()[-1] * data.size()[-2]))\n",
    "    image_targets = (image_target, shuffled_image_target, lam)\n",
    "    genre_targets = (genre_target, shuffled_genre_target, lam)\n",
    "\n",
    "    return new_data, image_targets, genre_targets\n",
    "\n",
    "def fmix(data, image_target, genre_target, alpha, decay_power, shape, max_soft=0.0, reformulate=False):\n",
    "    indices = torch.randperm(data.size(0))\n",
    "    shuffled_data = data[indices]\n",
    "    shuffled_image_target = image_target[indices]\n",
    "    shuffled_genre_target = genre_target[indices]\n",
    "\n",
    "    lam, mask = sample_mask(alpha, decay_power, shape, max_soft, reformulate)\n",
    "    x1 = torch.from_numpy(mask).to(device)*data\n",
    "    x2 = torch.from_numpy(1-mask).to(device)*shuffled_data\n",
    "    image_targets=(image_target, shuffled_image_target, lam)\n",
    "    genre_targets = (genre_target, shuffled_genre_target, lam)\n",
    "\n",
    "    return (x1+x2).float(), image_targets, genre_targets\n",
    "\n",
    "def mixup(data, image_target, genre_target, alpha=1.):\n",
    "    indices = torch.randperm(data.size(0))\n",
    "    shuffled_image_target = image_target[indices]\n",
    "    shuffled_genre_target = genre_target[indices]\n",
    "\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1\n",
    "\n",
    "    mixed_data = lam * data + (1 - lam) * data[indices, :]\n",
    "    image_targets = (image_target, shuffled_image_target, lam)\n",
    "    genre_targets = (genre_target, shuffled_genre_target, lam)\n",
    "\n",
    "    return mixed_data, image_targets, genre_targets"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-11T09:29:32.924348600Z",
     "start_time": "2023-12-11T09:29:32.873740800Z"
    }
   },
   "id": "f7c224799d1f9d32"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "233553a3679836f5"
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [],
   "source": [
    "def train(model, model_name, optimizer, train_loader, test_loader, scheduler, device):\n",
    "    model.to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "    best_score = 0\n",
    "    best_model = None\n",
    "\n",
    "    for epoch in range(1,CFG['EPOCHS']+1):\n",
    "        if model_name != 'StackingModel':\n",
    "            model.train()\n",
    "        train_loss = []\n",
    "        for img, label, genre_label in tqdm(train_loader):\n",
    "            img, label = img.float().to(device), label.to(device)\n",
    "\n",
    "            mix_decision = np.random.rand()\n",
    "            if mix_decision < 0.25:\n",
    "                img, label, _ = cutmix(img, label, genre_label, 1.)\n",
    "            elif 0.25 <= mix_decision < 0.5:\n",
    "                img_size = CFG['IMG_SIZE_L'] if 'l16' in model_name else CFG['IMG_SIZE_B']\n",
    "                img, label, _ = fmix(img, label, genre_label, alpha=1., decay_power=5., shape=(img_size,img_size))\n",
    "            elif 0.5 <= mix_decision <0.75:\n",
    "                img, label, _ = mixup(img, label, genre_label, 1.)\n",
    "                \n",
    "                \n",
    "            print(label)\n",
    "            print(genre_label)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            model_pred = model(img)\n",
    "\n",
    "            if mix_decision < 0.75:\n",
    "                loss = criterion(model_pred, label[0]) * label[2] + criterion(model_pred, label[1]) * (1. - label[2])\n",
    "            else:\n",
    "                loss = criterion(model_pred, label)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss.append(loss.item())\n",
    "\n",
    "        tr_loss = np.mean(train_loss)\n",
    "\n",
    "        val_loss, val_score = validation(model, criterion, test_loader, device)\n",
    "\n",
    "        print(f'Epoch [{epoch}], Train Loss : [{tr_loss:.5f}] Val Loss : [{val_loss:.5f}] Val F1 Score : [{val_score:.5f}]')\n",
    "\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "        if best_score < val_score:\n",
    "            torch.save(model.state_dict(), f'{model_name}.pt')\n",
    "            best_model = model\n",
    "            best_score = val_score\n",
    "\n",
    "    return best_model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-11T09:29:33.486206900Z",
     "start_time": "2023-12-11T09:29:33.470679900Z"
    }
   },
   "id": "a4f902fcb96e2af8"
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [],
   "source": [
    "# genre 정보를 활용한 모델을 위한 train 함수\n",
    "def genre_train(model, model_name, optimizer, train_loader, test_loader, scheduler, device):\n",
    "    model.to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "    best_score = 0\n",
    "    best_model = None\n",
    "\n",
    "    for epoch in range(1,CFG['EPOCHS']+1):\n",
    "        model.train()\n",
    "        train_loss, train_artist_loss, train_genre_loss = [], [], []\n",
    "        for img, label, genre_label in tqdm(train_loader):\n",
    "            img, label, genre_label = img.float().to(device), label.to(device), genre_label.float().to(device)\n",
    "\n",
    "            mix_decision = np.random.rand()\n",
    "            if mix_decision < 0.25:\n",
    "                img, label, genre_label = cutmix(img, label, genre_label, 1.)\n",
    "            elif 0.25 <= mix_decision < 0.5:\n",
    "                img_size = CFG['IMG_SIZE_L'] if 'l16' in model_name else CFG['IMG_SIZE_B']\n",
    "                img, label, genre_label = fmix(img, label, genre_label, alpha=1., decay_power=5., shape=(img_size,img_size))\n",
    "            elif 0.5 <= mix_decision <0.75:\n",
    "                img, label, genre_label = mixup(img, label, genre_label, 1.)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            if mix_decision < 0.75:\n",
    "                model_pred, genre_pred = model(img, genre_label[0] * genre_label[2] + genre_label[1] * (1. - genre_label[2]))\n",
    "                artist_loss = criterion(model_pred, label[0]) * label[2] + criterion(model_pred, label[1]) * (1. - label[2])\n",
    "                genre_loss = criterion(genre_pred, genre_label[0]) * genre_label[2] + criterion(genre_pred, genre_label[1]) * (1. - genre_label[2])\n",
    "            else:\n",
    "                model_pred, genre_pred = model(img, genre_label)\n",
    "                artist_loss = criterion(model_pred, label)\n",
    "                genre_loss = criterion(genre_pred, genre_label)\n",
    "\n",
    "            loss = artist_loss*0.7 + genre_loss*0.3\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss.append(loss.item())\n",
    "            train_artist_loss.append(artist_loss.item())\n",
    "            train_genre_loss.append(genre_loss.item())\n",
    "\n",
    "        tr_loss, tr_artist_loss, tr_genre_loss = np.mean(train_loss), np.mean(train_artist_loss), np.mean(train_genre_loss)\n",
    "\n",
    "        val_loss, val_artist_loss, val_genre_loss, val_score = genre_validation(model, criterion, test_loader, device)\n",
    "\n",
    "        print(f'Epoch [{epoch}], Train Loss: [{tr_loss:.5f}], Artist Loss: [{tr_artist_loss:.5f}], Genre Loss: [{tr_genre_loss:.5f}]'\n",
    "              f'Val Loss: [{val_loss:.5f}], Artist Loss: [{val_artist_loss:.5f}], Genre Loss: [{val_genre_loss:.5f}], Val F1 Score: [{val_score:.5f}]')\n",
    "\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "        if best_score < val_score:\n",
    "            torch.save(model.state_dict(), f'{model_name}.pt')\n",
    "            best_model = model\n",
    "            best_score = val_score\n",
    "\n",
    "    return best_model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-11T09:29:33.665635500Z",
     "start_time": "2023-12-11T09:29:33.648679200Z"
    }
   },
   "id": "be871ccae85a5ce9"
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [],
   "source": [
    "def competition_metric(true, pred):\n",
    "    return f1_score(true, pred, average=\"macro\")\n",
    "\n",
    "\n",
    "def validation(model, criterion, test_loader, device):\n",
    "    model.eval()\n",
    "\n",
    "    model_preds = []\n",
    "    true_labels = []\n",
    "\n",
    "    val_loss = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for img, label, _ in tqdm(test_loader):\n",
    "            img, label = img.float().to(device), label.to(device)\n",
    "\n",
    "            model_pred = model(img)\n",
    "\n",
    "            loss = criterion(model_pred, label)\n",
    "\n",
    "            val_loss.append(loss.item())\n",
    "\n",
    "            model_preds += model_pred.argmax(1).detach().cpu().numpy().tolist()\n",
    "            true_labels += label.detach().cpu().numpy().tolist()\n",
    "\n",
    "    val_f1 = competition_metric(true_labels, model_preds)\n",
    "    return np.mean(val_loss), val_f1\n",
    "\n",
    "# genre 정보를 활용한 모델을 위한 validation 함수\n",
    "def genre_validation(model, criterion, test_loader, device):\n",
    "    model.eval()\n",
    "\n",
    "    model_preds = []\n",
    "    true_labels = []\n",
    "\n",
    "    val_loss, val_artist_loss, val_genre_loss = [], [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for img, label, genre_label in tqdm(iter(test_loader)):\n",
    "            img, label, genre_label = img.float().to(device), label.to(device), genre_label.float().to(device)\n",
    "\n",
    "            model_pred, genre_pred = model(img)\n",
    "\n",
    "            artist_loss = criterion(model_pred, label)\n",
    "            genre_loss = criterion(genre_pred, genre_label)\n",
    "            loss = artist_loss*0.7 + genre_loss*0.3\n",
    "\n",
    "            val_loss.append(loss.item())\n",
    "            val_artist_loss.append(artist_loss.item())\n",
    "            val_genre_loss.append(genre_loss.item())\n",
    "\n",
    "            model_preds += model_pred.argmax(1).detach().cpu().numpy().tolist()\n",
    "            true_labels += label.detach().cpu().numpy().tolist()\n",
    "\n",
    "    val_f1 = competition_metric(true_labels, model_preds)\n",
    "    return np.mean(val_loss), np.mean(val_artist_loss), np.mean(val_genre_loss), val_f1"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-11T09:29:33.868052900Z",
     "start_time": "2023-12-11T09:29:33.821973Z"
    }
   },
   "id": "164bcec7df4a328a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 모델 훈련 시작"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cb9adbd533ac385d"
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [],
   "source": [
    "model_list = [BaseModel(), ViTModelB16(), ViTModelL16(), ViTModelB16Genre(), ViTModelL16Genre()]\n",
    "model_name = ['EfficientNet_v2_m', 'VisionTransformer_b16', 'VisionTransformer_l16', 'VisionTransformer_b16_genre', 'VisionTransformer_l16_genre']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-11T09:29:49.297286800Z",
     "start_time": "2023-12-11T09:29:34.135831500Z"
    }
   },
   "id": "b27d89bbd431773c"
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [],
   "source": [
    "optimizer_list = []\n",
    "scheduler_list = []\n",
    "for idx, model in enumerate(model_list):\n",
    "    optimizer_list.append(torch.optim.AdamW(params = model.parameters(), lr = CFG[\"LEARNING_RATE\"], weight_decay=0.01))\n",
    "    scheduler_list.append(\n",
    "        get_cosine_schedule_with_warmup(\n",
    "            optimizer=optimizer_list[idx],\n",
    "            num_warmup_steps=len(train_loader_b) * 20,\n",
    "            num_training_steps=len(train_loader_b) * CFG[\"EPOCHS\"]\n",
    "        )\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-11T09:29:49.476510900Z",
     "start_time": "2023-12-11T09:29:49.300829500Z"
    }
   },
   "id": "3a3ce19a3e11591f"
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################\n",
      "EfficientNet_v2_m Model Train Start\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/1182 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "59aceca184ea427e89e473880d5193d3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 4, 10, 34, 17], device='cuda:0', dtype=torch.int32)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], dtype=torch.float64)\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacty of 8.00 GiB of which 0 bytes is free. Of the allocated memory 7.22 GiB is allocated by PyTorch, and 12.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mOutOfMemoryError\u001B[0m                          Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[79], line 15\u001B[0m\n\u001B[0;32m     13\u001B[0m     genre_train(model, model_name[idx], optimizer_list[idx], train_loader, val_loader, scheduler_list[idx], device)\n\u001B[0;32m     14\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m---> 15\u001B[0m     \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel_name\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer_list\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mval_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mscheduler_list\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[74], line 31\u001B[0m, in \u001B[0;36mtrain\u001B[1;34m(model, model_name, optimizer, train_loader, test_loader, scheduler, device)\u001B[0m\n\u001B[0;32m     27\u001B[0m \u001B[38;5;28mprint\u001B[39m(genre_label)\n\u001B[0;32m     29\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[1;32m---> 31\u001B[0m model_pred \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimg\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     33\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m mix_decision \u001B[38;5;241m<\u001B[39m \u001B[38;5;241m0.75\u001B[39m:\n\u001B[0;32m     34\u001B[0m     loss \u001B[38;5;241m=\u001B[39m criterion(model_pred, label[\u001B[38;5;241m0\u001B[39m]) \u001B[38;5;241m*\u001B[39m label[\u001B[38;5;241m2\u001B[39m] \u001B[38;5;241m+\u001B[39m criterion(model_pred, label[\u001B[38;5;241m1\u001B[39m]) \u001B[38;5;241m*\u001B[39m (\u001B[38;5;241m1.\u001B[39m \u001B[38;5;241m-\u001B[39m label[\u001B[38;5;241m2\u001B[39m])\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\artists_classification\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\artists_classification\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1522\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1523\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1524\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1525\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1526\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1527\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1529\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1530\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "Cell \u001B[1;32mIn[47], line 12\u001B[0m, in \u001B[0;36mBaseModel.forward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m     11\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[1;32m---> 12\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackbone\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     13\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m x\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\artists_classification\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\artists_classification\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1522\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1523\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1524\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1525\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1526\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1527\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1529\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1530\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\artists_classification\\lib\\site-packages\\torchvision\\models\\efficientnet.py:343\u001B[0m, in \u001B[0;36mEfficientNet.forward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m    342\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m--> 343\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_forward_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\artists_classification\\lib\\site-packages\\torchvision\\models\\efficientnet.py:333\u001B[0m, in \u001B[0;36mEfficientNet._forward_impl\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m    332\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_forward_impl\u001B[39m(\u001B[38;5;28mself\u001B[39m, x: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m--> 333\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfeatures\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    335\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mavgpool(x)\n\u001B[0;32m    336\u001B[0m     x \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mflatten(x, \u001B[38;5;241m1\u001B[39m)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\artists_classification\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\artists_classification\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1522\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1523\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1524\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1525\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1526\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1527\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1529\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1530\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\artists_classification\\lib\\site-packages\\torch\\nn\\modules\\container.py:215\u001B[0m, in \u001B[0;36mSequential.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    213\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m):\n\u001B[0;32m    214\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m:\n\u001B[1;32m--> 215\u001B[0m         \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[43mmodule\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m    216\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28minput\u001B[39m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\artists_classification\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\artists_classification\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1522\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1523\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1524\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1525\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1526\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1527\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1529\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1530\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\artists_classification\\lib\\site-packages\\torch\\nn\\modules\\container.py:215\u001B[0m, in \u001B[0;36mSequential.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    213\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m):\n\u001B[0;32m    214\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m:\n\u001B[1;32m--> 215\u001B[0m         \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[43mmodule\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m    216\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28minput\u001B[39m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\artists_classification\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\artists_classification\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1522\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1523\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1524\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1525\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1526\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1527\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1529\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1530\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\artists_classification\\lib\\site-packages\\torchvision\\models\\efficientnet.py:164\u001B[0m, in \u001B[0;36mMBConv.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    163\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m--> 164\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mblock\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m    165\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39muse_res_connect:\n\u001B[0;32m    166\u001B[0m         result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstochastic_depth(result)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\artists_classification\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\artists_classification\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1522\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1523\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1524\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1525\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1526\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1527\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1529\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1530\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\artists_classification\\lib\\site-packages\\torch\\nn\\modules\\container.py:215\u001B[0m, in \u001B[0;36mSequential.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    213\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m):\n\u001B[0;32m    214\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m:\n\u001B[1;32m--> 215\u001B[0m         \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[43mmodule\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m    216\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28minput\u001B[39m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\artists_classification\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\artists_classification\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1522\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1523\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1524\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1525\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1526\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1527\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1529\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1530\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\artists_classification\\lib\\site-packages\\torch\\nn\\modules\\container.py:215\u001B[0m, in \u001B[0;36mSequential.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    213\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m):\n\u001B[0;32m    214\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m:\n\u001B[1;32m--> 215\u001B[0m         \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[43mmodule\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m    216\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28minput\u001B[39m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\artists_classification\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\artists_classification\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1522\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1523\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1524\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1525\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1526\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1527\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1529\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1530\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\artists_classification\\lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:171\u001B[0m, in \u001B[0;36m_BatchNorm.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    164\u001B[0m     bn_training \u001B[38;5;241m=\u001B[39m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrunning_mean \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m) \u001B[38;5;129;01mand\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrunning_var \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[0;32m    166\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    167\u001B[0m \u001B[38;5;124;03mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001B[39;00m\n\u001B[0;32m    168\u001B[0m \u001B[38;5;124;03mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001B[39;00m\n\u001B[0;32m    169\u001B[0m \u001B[38;5;124;03mused for normalization (i.e. in eval mode when buffers are not None).\u001B[39;00m\n\u001B[0;32m    170\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m--> 171\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbatch_norm\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    172\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    173\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;66;43;03m# If buffers are not to be tracked, ensure that they won't be updated\u001B[39;49;00m\n\u001B[0;32m    174\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrunning_mean\u001B[49m\n\u001B[0;32m    175\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mnot\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtraining\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrack_running_stats\u001B[49m\n\u001B[0;32m    176\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    177\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrunning_var\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mnot\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtraining\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrack_running_stats\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    178\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    179\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    180\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbn_training\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    181\u001B[0m \u001B[43m    \u001B[49m\u001B[43mexponential_average_factor\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    182\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43meps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    183\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\artists_classification\\lib\\site-packages\\torch\\nn\\functional.py:2478\u001B[0m, in \u001B[0;36mbatch_norm\u001B[1;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001B[0m\n\u001B[0;32m   2475\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m training:\n\u001B[0;32m   2476\u001B[0m     _verify_batch_size(\u001B[38;5;28minput\u001B[39m\u001B[38;5;241m.\u001B[39msize())\n\u001B[1;32m-> 2478\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbatch_norm\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   2479\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbias\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrunning_mean\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrunning_var\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtraining\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmomentum\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43meps\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackends\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcudnn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menabled\u001B[49m\n\u001B[0;32m   2480\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mOutOfMemoryError\u001B[0m: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacty of 8.00 GiB of which 0 bytes is free. Of the allocated memory 7.22 GiB is allocated by PyTorch, and 12.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "for idx, model in enumerate(model_list):\n",
    "    print('#'*50)\n",
    "    print(f'{model_name[idx]} Model Train Start')\n",
    "\n",
    "    if 'l16' in model_name[idx]:\n",
    "        train_loader = train_loader_l\n",
    "        val_loader = val_loader_l\n",
    "    else:\n",
    "        train_loader = train_loader_b\n",
    "        val_loader = val_loader_b\n",
    "\n",
    "    if model_name[idx].endswith('genre'):\n",
    "        genre_train(model, model_name[idx], optimizer_list[idx], train_loader, val_loader, scheduler_list[idx], device)\n",
    "    else:\n",
    "        train(model, model_name[idx], optimizer_list[idx], train_loader, val_loader, scheduler_list[idx], device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-11T09:29:55.626542700Z",
     "start_time": "2023-12-11T09:29:49.479026200Z"
    }
   },
   "id": "6d8e19b8ff95e35e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# stacking ensemble"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ef8c73661888dd05"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class StackingModel(nn.Module):\n",
    "    def __init__(self, num_classes=len(le.classes_)):\n",
    "        super(StackingModel, self).__init__()\n",
    "        self.efficientnet = BaseModel()\n",
    "        self.vit_b16 = ViTModelB16()\n",
    "        self.vit_b16_genre = ViTModelB16Genre()\n",
    "        self.vit_l16 = ViTModelL16()\n",
    "        self.vit_l16_genre = ViTModelL16Genre()\n",
    "\n",
    "        self.efficientnet.load_state_dict(torch.load('./EfficientNet_v2_m.pt', map_location=device))\n",
    "        self.vit_b16.load_state_dict(torch.load('./VisionTransformer_b16.pt', map_location=device))\n",
    "        self.vit_b16_genre.load_state_dict(torch.load('./VisionTransformer_b16_genre.pt', map_location=device))\n",
    "        self.vit_l16.load_state_dict(torch.load('./VisionTransformer_l16.pt', map_location=device))\n",
    "        self.vit_l16_genre.load_state_dict(torch.load('./VisionTransformer_l16_genre.pt', map_location=device))\n",
    "\n",
    "        self.efficientnet.requires_grad_(False)\n",
    "        self.vit_b16.requires_grad_(False)\n",
    "        self.vit_b16_genre.requires_grad_(False)\n",
    "        self.vit_l16.requires_grad_(False)\n",
    "        self.vit_l16_genre.requires_grad_(False)\n",
    "\n",
    "        self.layer_norm = nn.LayerNorm(num_classes*5, eps=1e-6)\n",
    "\n",
    "        self.dense = nn.Linear(in_features=num_classes*5, out_features=num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_l = torchvision.transforms.functional.resize(x, [CFG['IMG_SIZE_L'], CFG['IMG_SIZE_L']])\n",
    "\n",
    "        x1 = self.efficientnet(x)\n",
    "        x2 = self.vit_b16(x)\n",
    "        x3, _ = self.vit_b16_genre(x)\n",
    "        x4 = self.vit_l16(x_l)\n",
    "        x5, _ = self.vit_l16_genre(x_l)\n",
    "\n",
    "        x = torch.cat([x1, x2, x3, x4, x5], dim=1)\n",
    "        x = self.layer_norm(x)\n",
    "\n",
    "        x = self.dense(x)\n",
    "\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-11T09:29:55.618428400Z"
    }
   },
   "id": "7c7ffe2766f77b85"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = StackingModel()\n",
    "optimizer = torch.optim.AdamW(params=model.parameters(), lr=CFG[\"LEARNING_RATE\"], weight_decay=0.01)\n",
    "scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=len(train_loader_b) * 20,\n",
    "    num_training_steps=len(train_loader_b) * CFG[\"EPOCHS\"]\n",
    ")\n",
    "\n",
    "infer_model = train(model, 'StackingModel', optimizer, train_loader_b, val_loader_b, scheduler, device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-11T09:29:55.621484700Z"
    }
   },
   "id": "bbb44cb736c40a2f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# inference"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c371dbb46cd5ee8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(os.path.join(data_path, './test.csv'))\n",
    "test_df.head()\n",
    "\n",
    "test_img_paths = get_data(test_df, infer=True)\n",
    "\n",
    "test_dataset = CustomDataset(test_img_paths, None, None, test_transform_b)\n",
    "test_loader = DataLoader(test_dataset, batch_size=CFG['BATCH_SIZE'], shuffle=False, num_workers=4)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-11T09:29:55.625032800Z"
    }
   },
   "id": "71544a37be8725ba"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def inference(model, test_loader, device):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    model_preds = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, img in enumerate(tqdm(iter(test_loader))):\n",
    "            img = img.float().to(device)\n",
    "\n",
    "            model_pred = model(img)\n",
    "\n",
    "\n",
    "            model_preds += model_pred.argmax(1).detach().cpu().numpy().tolist()\n",
    "\n",
    "    print('Done.')\n",
    "    return model_preds"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-11T09:29:55.626542700Z"
    }
   },
   "id": "9bdcbe90063df27d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "preds = inference(infer_model, test_loader, device)\n",
    "preds = le.inverse_transform(preds)  # LabelEncoder로 변환 된 Label을 다시 화가 이름으로 변환\n",
    "\n",
    "# ## Submit\n",
    "submit = pd.read_csv(os.path.join(data_path, './sample_submission.csv'))\n",
    "submit['artist'] = preds\n",
    "submit.to_csv(os.path.join(data_path, './submit.csv'), index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-11T09:29:55.628571400Z"
    }
   },
   "id": "d9c6488a04eb9760"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "a2c3f7891692c7ad"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
